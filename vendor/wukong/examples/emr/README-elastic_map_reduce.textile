
# Download the Amazon elastic-mapreduce runner from http://elasticmapreduce.s3.amazonaws.com/elastic-mapreduce-ruby.zip

# Create a bucket and path to hold your EMR logs, scripts and other ephemera. For instance you might choose 'emr.yourdomain.com' as the bucket and '/wukong' as a scoping path within that bucket. In that case you will refer to it with a path like s3n://emr.yourdomain.com/wukong (see notes below about s3n:// vs. s3:// URLs).

# Copy the contents of wukong/examples/emr/dot_wukong_dir to ~/.wukong
# Edit emr.yaml -- it has instructions for the 





h3. s3n:// vs. s3:// URLs

Many external tools use a URI convention to address files in S3; they typically use the 's3://' scheme, which makes a lot of sense:
  s3://emr.yourcompany.com/wukong/happy_job_1/logs/whatever-20100808.log

Hadoop can maintain an HDFS on the Amazon S3: it uses a block structure and has optimizations for streaming, no file size limitation, and other goodness. However, only hadoop tools can interpret the contents of those blocks -- to everything else it just looks like a soup of blocks labelled block_-8675309 and so forth.  Hadoop unfortunately chose the 's3://' scheme for URIs in this filesystem:
  s3://s3hdfs.yourcompany.com/path/to/data

Hadoop is happy to read s3 native files -- 'native' as in, you can look at them with a browser and upload them an download them with any S3 tool out there. There's a 5GB limit on file size, and in some cases a performance hit (but not in our experience enough to worry about).  You refer to these files with the 's3n://' scheme ('n' as in 'native'):
  s3n://emr.yourcompany.com/wukong/happy_job_1/code/happy_job_1-mapper.rb
  s3n://emr.yourcompany.com/wukong/happy_job_1/code/happy_job_1-reducer.rb
  s3n://emr.yourcompany.com/wukong/happy_job_1/logs/whatever-20100808.log

Wukong will coerce things to the right scheme when it knows what that scheme should be (eg. code should be s3n://). It will otherwise leave the path alone. Specifically, if you use a URI scheme for input and output paths you must use 's3n://' for normal s3 files.